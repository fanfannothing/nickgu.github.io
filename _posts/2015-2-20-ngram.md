---
layout: post
title: N-gram笔记
---
## 背景及主要问题

### 香农游戏和语言模型问题

香农游戏，即在给定前N个词的情况下，需要尽可能准确预测下一个词。这是语言模型一个比较形象的描述，通常我们所指的语言模型，说白了就是计算某语言片段在上下文的概率。可以认为分别针对两类典型的实际问题：

* 给定前文，给出下一个词的概率分布，也即需要计算
$$P(w_n|w_0, ..., w_n-1)$$
* 给定一篇完整的文章，计算其在某语言生成模型下的似然概率，也即需要计算
$$P(w_0, ..., w_n)$$

### ngram方法
对于上述问题，ngram是一种经典的解决方法，其通过计算\(P()\)

## 评测方式


## 平滑方法

对于ngram，最大的问题是数据稀疏上的处理。书中提到一些经典的处理方法

* +1平滑
* Lidstone法则
* 留存估计（hold-out data）
* 删除估计法。
* Good-Turing估计

另外对于ngram的方法的效果检验，可以通过和一般机器学习测试的方法一样，训练集用于统计，测试集用于测试。
测试方法可以使用似然估计等（对每个词根据上下文计算概率，哪个模型大，说明那个模型模拟的好）。

留存估计是指留一套holdout data，对于每种出现的频率情况（c=1, 2, ..）分别统计这个种类在留存数据集上的概率，作为对应种类统计的概率估计。


## Toy实现

## 参考资料
